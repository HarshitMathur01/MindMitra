"""
MindMitra Psychology Workflow v2 â€” Modular Architecture
========================================================

Architecture:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                    UserContext (shared JSON)                  â”‚
  â”‚  Every module reads from and writes results back to this     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚          â”‚          â”‚          â”‚          â”‚
     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Memory   â”‚ â”‚ NLP  â”‚ â”‚Culturalâ”‚ â”‚Psych   â”‚ â”‚  Technique   â”‚
     â”‚  System   â”‚ â”‚(Groq)â”‚ â”‚Context â”‚ â”‚Analysisâ”‚ â”‚  Selector    â”‚
     â”‚ (kept as  â”‚ â”‚      â”‚ â”‚ Module â”‚ â”‚(GLM)   â”‚ â”‚  (GLM)       â”‚
     â”‚  is)      â”‚ â”‚      â”‚ â”‚        â”‚ â”‚        â”‚ â”‚              â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                                                  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                                                  â”‚  Response   â”‚
                                                  â”‚  Generator  â”‚
                                                  â”‚  (GLM)      â”‚
                                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

External interface (process_user_chat / process_chat) is UNCHANGED.
"""

import os
import json
import time
import logging
import threading
import re
from typing import Dict, Any, Optional, List
from datetime import datetime, timezone
from copy import deepcopy
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage
from pydantic import BaseModel, Field
from supabase import create_client, Client
from memory_architecture import UniversalMemorySystem

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Logging
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
load_dotenv()


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  1. SHARED USER-CONTEXT JSON SCHEMA                         â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def create_empty_user_context(
    user_id: str = "anonymous",
    session_id: str = None,
    user_message: str = "",
) -> Dict[str, Any]:
    """
    Canonical JSON envelope that every module reads from / writes to.
    Nothing leaves or enters the pipeline except through this structure.
    """
    return {
        # â”€â”€ identity â”€â”€
        "user_id": user_id,
        "session_id": session_id,
        "timestamp": datetime.now(timezone.utc).isoformat(),

        # â”€â”€ raw input â”€â”€
        "user_message": user_message,
        "voice_analysis": {},                       # optional voice data

        # â”€â”€ session history (populated by caller / memory fetch) â”€â”€
        "session_context": {
            "recent_messages": [],
            "conversation_summary": {},
            "session_memories": {
                "procedural": [],
                "semantic": [],
                "episodic": [],
            },
            "user_activities": [],
            "user_patterns": {},
        },

        # â”€â”€ NLP analysis  (written by Groq NLP module) â”€â”€
        "nlp_analysis": {
            "emotions": {},                         # {joy: 0.1, sadness: 0.7, â€¦}
            "primary_emotion": "",
            "sentiment": {
                "score": 0.0,                       # -1 â€¦ +1
                "label": "neutral",                 # positive / negative / neutral / mixed
            },
            "intensity": 0.0,                       # 0 â€¦ 1
            "key_phrases": [],
            "language_detected": "en",
            "urgency_flag": False,
        },

        # â”€â”€ cultural context (written by cultural module) â”€â”€
        "cultural_context": {
            "language_style": "casual",             # formal / casual / hindi-mixed
            "hindi_english_ratio": 0.0,             # 0 = pure English, 1 = pure Hindi
            "code_switching_detected": False,
            "cultural_sensitivity_flags": [],        # e.g. "parental_pressure", "exam_stress"
            "communication_pattern": "",
            "regional_context": "",
            "formality_level": "medium",            # low / medium / high
        },

        # â”€â”€ psychological analysis  (written by GLM Agent 1) â”€â”€
        "psychological_analysis": {
            "emotional_state": "",
            "stress_categories": [],
            "risk_assessment": "low",
            "coping_assessment": "",
            "intervention_priority": "supportive",
            "psychological_insights": [],
            "cultural_pressures": "",
        },

        # â”€â”€ technique selection  (written by GLM Agent 2) â”€â”€
        "technique_selection": {
            "primary_technique": "",                # CBT / ACT / MBCT / â€¦
            "therapeutic_approach": "",
            "activity_recommendations": [],
            "rationale": "",
        },

        # â”€â”€ final output â”€â”€
        "ai_response": "",
        "response_generated": False,
    }


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  2. GROQ NLP â€” Emotion & Sentiment Analysis                 â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class GroqNLPModule:
    """
    Lightweight emotion / sentiment analysis via Groq (llama/mixtral).
    Handles token-limit errors with automatic truncation & retry.
    """

    # Groq free-tier context sizes by model
    _MODEL_TOKEN_LIMITS = {
        "qwen/qwen3-32b": 4_096,
        "moonshotai/kimi-k2-instruct-0905": 4096,
        "meta-llama/llama-4-scout-17b-16e-instruct": 8_192,
       # "mixtral-8x7b-32768": 32_768,
    }

    def __init__(self, api_key: str = None, model: str = "qwen/qwen3-32b"):
        self.api_key = api_key or os.getenv("GROQ_API_KEY")
        if not self.api_key:
            logger.warning("âš ï¸ [GROQ-NLP] GROQ_API_KEY not set â€” NLP module disabled")
            self.client = None
            return

        try:
            from groq import Groq
            self.client = Groq(api_key=self.api_key)
            self.model = model
            self._max_input_chars = self._MODEL_TOKEN_LIMITS.get(model, 8_192) * 3  # ~3 chars/token rough est
            logger.info(f"âœ… [GROQ-NLP] Initialised with model={model}")
        except ImportError:
            logger.warning("âš ï¸ [GROQ-NLP] `groq` package not installed â€” NLP module disabled")
            self.client = None
        except Exception as e:
            logger.error(f"âŒ [GROQ-NLP] Init failed: {e}")
            self.client = None

    # â”€â”€ public entry â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def analyse(self, user_context: Dict[str, Any]) -> Dict[str, Any]:
        """Run emotion + sentiment analysis; write results into user_context['nlp_analysis']."""
        if not self.client:
            logger.info("[GROQ-NLP] Skipped (client not available)")
            return user_context

        text = user_context.get("user_message", "")
        # Include last 3 messages for conversational context
        recent = user_context["session_context"].get("recent_messages", [])[-3:]
        history_snippet = " | ".join(
            f"{m.get('role','?')}: {m.get('content','')[:120]}" for m in recent
        )

        prompt = self._build_prompt(text, history_snippet)
        raw = self._call_groq(prompt)
        parsed = self._parse_response(raw)
        user_context["nlp_analysis"] = parsed
        logger.info(f"âœ… [GROQ-NLP] Emotion={parsed.get('primary_emotion')}, Sentiment={parsed['sentiment']['label']}")
        return user_context

    # â”€â”€ internals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _build_prompt(self, text: str, history: str) -> str:
        return f"""Analyse the following user message for a mental-health chatbot.  
Return ONLY valid JSON (no markdown fences) with exactly these keys:

{{
  "emotions": {{"joy": 0.0, "sadness": 0.0, "anger": 0.0, "fear": 0.0, "surprise": 0.0, "disgust": 0.0, "trust": 0.0, "anticipation": 0.0}},
  "primary_emotion": "<strongest emotion name>",
  "sentiment": {{"score": <float -1 to 1>, "label": "<positive|negative|neutral|mixed>"}},
  "intensity": <float 0 to 1>,
  "key_phrases": ["<phrase1>", "<phrase2>"],
  "language_detected": "<en|hi|hinglish>",
  "urgency_flag": <true if crisis/self-harm indicators else false>
}}

Recent conversation context: {history[:600]}

User message: \"{text[:1500]}\"

JSON:"""

    def _call_groq(self, prompt: str, _retry: int = 0) -> str:
        """Call Groq with automatic truncation on token-limit errors."""
        try:
            resp = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=400,
            )
            return resp.choices[0].message.content.strip()

        except Exception as e:
            err_str = str(e).lower()
            # Handle token limit exceeded â€” truncate and retry once
            if ("token" in err_str or "context_length" in err_str or "rate_limit" in err_str) and _retry < 2:
                logger.warning(f"âš ï¸ [GROQ-NLP] Token/rate limit hit (attempt {_retry+1}), truncating...")
                truncated = prompt[: len(prompt) // 2]
                return self._call_groq(truncated, _retry + 1)
            logger.error(f"âŒ [GROQ-NLP] API call failed: {e}")
            return "{}"

    def _parse_response(self, raw: str) -> Dict:
        """Robust JSON parse with fallback defaults."""
        defaults = {
            "emotions": {},
            "primary_emotion": "unknown",
            "sentiment": {"score": 0.0, "label": "neutral"},
            "intensity": 0.0,
            "key_phrases": [],
            "language_detected": "en",
            "urgency_flag": False,
        }
        if not raw:
            return defaults
        try:
            # Strip markdown fences if the model wraps them
            cleaned = re.sub(r"```(?:json)?", "", raw).strip().rstrip("`")
            parsed = json.loads(cleaned)
            # Merge with defaults so no key is ever missing
            for k, v in defaults.items():
                if k not in parsed:
                    parsed[k] = v
            return parsed
        except json.JSONDecodeError:
            logger.warning("[GROQ-NLP] Failed to parse JSON, using defaults")
            return defaults


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  3. CULTURAL CONTEXT & LANGUAGE STYLE MODULE                 â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class CulturalContextModule:
    """
    Rule-based + lightweight LLM analysis for:
      â€¢ Hindi / Hinglish detection & code-switching level
      â€¢ Formality level
      â€¢ Cultural sensitivity flags (exam stress, parental pressure, â€¦)
      â€¢ Communication pattern classification
    Uses the NLP analysis already present in user_context and optionally
    calls Groq for deeper classification (kept cheap â€” single short call).
    """

    # Common Hindi / Hinglish markers
    _HINDI_MARKERS = {
        "yaar", "bhai", "didi", "maa", "papa", "ghar", "padhai", "exam",
        "nahi", "kya", "hai", "mein", "toh", "acha", "theek", "kuch",
        "kaise", "kyun", "bohot", "bahut", "zyada", "bilkul", "sach",
        "samajh", "dukh", "tension", "pareshan", "darr", "chinta",
        "mann", "dil", "sapna", "zindagi", "rishta", "shaadi",
        "arre", "haan", "naa", "abhi", "bas", "matlab", "lekin",
        "accha", "suno", "bata", "bol", "rona", "akela", "thak",
    }

    _CULTURAL_KEYWORDS = {
        "parental_pressure": ["parents", "papa", "maa", "mom", "dad", "family", "ghar", "expect", "disappoint", "proud"],
        "exam_stress": ["exam", "jee", "neet", "boards", "cgpa", "marks", "rank", "topper", "padhai", "result", "semester"],
        "career_anxiety": ["career", "job", "placement", "package", "future", "engineer", "doctor", "startup", "salary"],
        "social_pressure": ["friends", "relationship", "breakup", "lonely", "akela", "judge", "log kya kahenge", "society"],
        "identity_struggle": ["identity", "confused", "who am i", "purpose", "meaning", "self", "worth"],
        "marriage_pressure": ["shaadi", "marriage", "rishta", "arrange", "partner", "settle"],
        "mental_health_stigma": ["pagal", "crazy", "weak", "therapy", "stigma", "shame", "hide"],
    }

    def __init__(self, groq_nlp: Optional[GroqNLPModule] = None):
        self.groq_nlp = groq_nlp  # reuse same Groq client for optional deep analysis
        logger.info("âœ… [CULTURAL] Cultural context module initialised")

    def analyse(self, user_context: Dict[str, Any]) -> Dict[str, Any]:
        """Run cultural analysis; write results into user_context['cultural_context']."""
        text = user_context.get("user_message", "").lower()
        history = user_context["session_context"].get("recent_messages", [])

        result = {
            "language_style": self._detect_language_style(text),
            "hindi_english_ratio": self._compute_hindi_ratio(text),
            "code_switching_detected": False,
            "cultural_sensitivity_flags": self._detect_cultural_flags(text),
            "communication_pattern": self._detect_communication_pattern(text, history),
            "regional_context": self._infer_regional_context(text, history),
            "formality_level": self._detect_formality(text),
        }
        result["code_switching_detected"] = result["hindi_english_ratio"] > 0.1

        # If session history exists, enrich from patterns across messages
        if history:
            result = self._enrich_from_history(result, history)

        user_context["cultural_context"] = result
        logger.info(
            f"âœ… [CULTURAL] Style={result['language_style']}, "
            f"Hindi%={result['hindi_english_ratio']:.0%}, "
            f"Flags={result['cultural_sensitivity_flags']}"
        )
        return user_context

    # â”€â”€ detection helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _detect_language_style(self, text: str) -> str:
        words = set(text.split())
        hindi_count = len(words & self._HINDI_MARKERS)
        total = max(len(words), 1)
        ratio = hindi_count / total
        if ratio > 0.25:
            return "hindi-mixed"
        elif ratio > 0.08:
            return "hinglish"
        return "english"

    def _compute_hindi_ratio(self, text: str) -> float:
        words = text.split()
        if not words:
            return 0.0
        hindi_count = sum(1 for w in words if w.lower() in self._HINDI_MARKERS)
        return round(hindi_count / len(words), 3)

    def _detect_cultural_flags(self, text: str) -> List[str]:
        flags = []
        text_lower = text.lower()
        for flag, keywords in self._CULTURAL_KEYWORDS.items():
            if any(kw in text_lower for kw in keywords):
                flags.append(flag)
        return flags

    def _detect_communication_pattern(self, text: str, history: List) -> str:
        if len(text.split()) < 5:
            return "terse"
        elif len(text.split()) > 80:
            return "verbose"
        elif text.endswith("?"):
            return "questioning"
        elif any(w in text.lower() for w in ["feel", "feeling", "felt", "lagta", "mehsoos"]):
            return "emotionally_expressive"
        return "conversational"

    def _detect_formality(self, text: str) -> str:
        informal_markers = {"lol", "haha", "omg", "wtf", "bruh", "yaar", "arre", "bc", "mc"}
        formal_markers = {"sir", "ma'am", "respected", "kindly", "please", "would you"}
        words = set(text.lower().split())
        if words & informal_markers:
            return "low"
        if words & formal_markers:
            return "high"
        return "medium"

    def _infer_regional_context(self, text: str, history: List) -> str:
        # Simple keyword-based; can be enhanced
        all_text = text + " ".join(m.get("content", "") for m in history[-5:])
        all_lower = all_text.lower()
        if any(w in all_lower for w in ["kota", "jee", "iit", "coaching"]):
            return "competitive_exam_belt"
        if any(w in all_lower for w in ["bangalore", "bengaluru", "hyderabad", "pune", "it job", "startup"]):
            return "tech_hub"
        if any(w in all_lower for w in ["village", "gaon", "rural"]):
            return "rural"
        return "urban_metro"

    def _enrich_from_history(self, result: Dict, history: List) -> Dict:
        """Aggregate patterns across session history."""
        all_text = " ".join(m.get("content", "") for m in history if m.get("role") == "user")
        # Accumulate cultural flags from entire session
        session_flags = set(result["cultural_sensitivity_flags"])
        for flag, keywords in self._CULTURAL_KEYWORDS.items():
            if any(kw in all_text.lower() for kw in keywords):
                session_flags.add(flag)
        result["cultural_sensitivity_flags"] = list(session_flags)

        # Detect overall session language style (may differ from single message)
        session_hindi = self._compute_hindi_ratio(all_text)
        if session_hindi > result["hindi_english_ratio"]:
            result["hindi_english_ratio"] = round(
                (result["hindi_english_ratio"] + session_hindi) / 2, 3
            )
            if session_hindi > 0.2:
                result["language_style"] = "hindi-mixed"
        return result


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  4. GLM CONCURRENCY CONTROLLER                              â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class GLMController:
    """
    Wrapper around Google Gemini (GLM) that respects concurrent request
    limits using a threading Semaphore + exponential backoff on 429s.
    All GLM calls in the pipeline go through this controller.
    """

    def __init__(
        self,
        api_key: str = None,
        model: str = "gemini-2.5-flash-lite",
        max_concurrent: int = 3,
        max_retries: int = 3,
        base_backoff: float = 2.0,
    ):
        self.api_key = api_key or os.getenv("GOOGLE_API_KEY")
        if not self.api_key:
            raise ValueError("GOOGLE_API_KEY is required for GLM controller")

        self.model_name = model
        self._semaphore = threading.Semaphore(max_concurrent)
        self._max_retries = max_retries
        self._base_backoff = base_backoff
        self._lock = threading.Lock()

        # Shared LLM instance (thread-safe via semaphore)
        self.llm = ChatGoogleGenerativeAI(
            model=model,
            google_api_key=self.api_key,
            timeout=30,
            max_tokens=500,
            temperature=0.3,
            top_p=0.8,
            max_retries=1,
        )
        logger.info(f"âœ… [GLM] Controller ready â€” model={model}, max_concurrent={max_concurrent}")

    def invoke(self, messages: List, **kwargs) -> Any:
        """
        Thread-safe invoke with semaphore gating and retry on rate limits.
        """
        for attempt in range(self._max_retries):
            self._semaphore.acquire()
            try:
                result = self.llm.invoke(messages, **kwargs)
                return result
            except Exception as e:
                err = str(e).lower()
                if "429" in err or "rate" in err or "quota" in err or "resource_exhausted" in err:
                    wait = self._base_backoff * (2 ** attempt)
                    logger.warning(f"âš ï¸ [GLM] Rate limited (attempt {attempt+1}), backing off {wait:.1f}s")
                    time.sleep(wait)
                else:
                    logger.error(f"âŒ [GLM] Non-retryable error: {e}")
                    raise
            finally:
                self._semaphore.release()

        raise RuntimeError(f"[GLM] Exhausted {self._max_retries} retries due to rate limiting")


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  5. GLM AGENT 1 â€” Psychologist Analysis                     â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class PsychologistAnalysisAgent:
    """
    Reads the shared UserContext (NLP results, cultural context, memories,
    activities) and produces a clinical-grade psychological assessment.
    Writes into user_context['psychological_analysis'].
    """

    def __init__(self, glm: GLMController):
        self.glm = glm
        logger.info("âœ… [AGENT-1] Psychologist analysis agent ready")

    def run(self, user_context: Dict[str, Any]) -> Dict[str, Any]:
        logger.info("ğŸ§  [AGENT-1] Starting psychological analysis...")

        prompt = self._build_prompt(user_context)
        resp = self.glm.invoke([HumanMessage(content=prompt)])

        if not resp or not resp.content:
            raise ValueError("[AGENT-1] GLM returned empty response")

        parsed = self._parse_analysis(resp.content)
        user_context["psychological_analysis"] = parsed
        logger.info(
            f"âœ… [AGENT-1] Done â€” state={parsed.get('emotional_state','?')}, "
            f"priority={parsed.get('intervention_priority','?')}"
        )
        return user_context

    def _build_prompt(self, ctx: Dict) -> str:
        nlp = ctx.get("nlp_analysis", {})
        cultural = ctx.get("cultural_context", {})
        session = ctx.get("session_context", {})
        activities = session.get("user_activities", [])
        memories = session.get("session_memories", {})

        # Format memories compactly
        mem_lines = []
        for mtype in ("procedural", "semantic", "episodic"):
            for m in memories.get(mtype, [])[:4]:
                content = m.get("memory_content", m.get("content", ""))
                mem_lines.append(f"  [{mtype}] {content[:120]}")
        mem_block = "\n".join(mem_lines) if mem_lines else "No prior memories."

        # Format activities compactly
        act_lines = []
        for a in activities[:5]:
            atype = a.get("activity_type", "unknown")
            score = a.get("score", "?")
            insights = a.get("insights_generated", {})
            patterns = insights.get("key_patterns", [])
            act_lines.append(f"  {atype}: score={score}, patterns={patterns[:2]}")
        act_block = "\n".join(act_lines) if act_lines else "No activities yet."

        # Recent messages (last 5)
        recent = session.get("recent_messages", [])[-5:]
        conv_lines = []
        for m in recent:
            role = "User" if m.get("role") == "user" else "AI"
            conv_lines.append(f"  {role}: {m.get('content','')[:100]}")
        conv_block = "\n".join(conv_lines) if conv_lines else "New conversation."

        return f"""You are a clinical psychologist specialising in Indian youth (16-25).
Analyse this user and return ONLY valid JSON (no markdown fences) matching this schema:

{{
  "emotional_state": "<descriptive string>",
  "stress_categories": ["<Academic|Family|Social|Emotional|Identity|Career|Miscellaneous>"],
  "risk_assessment": "<low|moderate|high|crisis>",
  "coping_assessment": "<description of coping mechanisms & resilience>",
  "intervention_priority": "<immediate|supportive|long-term>",
  "psychological_insights": ["<insight1>", "<insight2>", "<insight3>"],
  "cultural_pressures": "<relevant Indian cultural/family/academic pressures>"
}}

â”€â”€â”€ DATA â”€â”€â”€

USER MESSAGE: "{ctx['user_message'][:800]}"

NLP ANALYSIS:
  Primary emotion: {nlp.get('primary_emotion','unknown')}
  Sentiment: {nlp.get('sentiment',{}).get('label','unknown')} ({nlp.get('sentiment',{}).get('score',0):.2f})
  Intensity: {nlp.get('intensity',0):.2f}
  Urgency: {nlp.get('urgency_flag', False)}
  Key phrases: {nlp.get('key_phrases',[])}

CULTURAL CONTEXT:
  Language style: {cultural.get('language_style','unknown')}
  Cultural flags: {cultural.get('cultural_sensitivity_flags',[])}
  Communication: {cultural.get('communication_pattern','unknown')}
  Formality: {cultural.get('formality_level','medium')}

SESSION MEMORIES:
{mem_block}

RECENT ACTIVITIES:
{act_block}

RECENT CONVERSATION:
{conv_block}

JSON:"""

    def _parse_analysis(self, raw: str) -> Dict:
        defaults = {
            "emotional_state": "needs assessment",
            "stress_categories": [],
            "risk_assessment": "low",
            "coping_assessment": "",
            "intervention_priority": "supportive",
            "psychological_insights": [],
            "cultural_pressures": "",
        }
        try:
            cleaned = re.sub(r"```(?:json)?", "", raw).strip().rstrip("`")
            parsed = json.loads(cleaned)
            for k, v in defaults.items():
                if k not in parsed:
                    parsed[k] = v
            return parsed
        except json.JSONDecodeError:
            logger.warning("[AGENT-1] JSON parse failed, using LLM text as insight")
            defaults["psychological_insights"] = [raw[:300]]
            return defaults


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  6. GLM AGENT 2 â€” Psychological Technique Selector           â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TechniqueSelectorAgent:
    """
    Reads the psychological analysis + NLP + cultural context from
    UserContext and selects the optimal therapeutic technique(s).
    Writes into user_context['technique_selection'].
    """

    def __init__(self, glm: GLMController):
        self.glm = glm
        logger.info("âœ… [AGENT-2] Technique selector agent ready")

    def run(self, user_context: Dict[str, Any]) -> Dict[str, Any]:
        logger.info("ğŸ’Š [AGENT-2] Selecting therapeutic technique...")

        prompt = self._build_prompt(user_context)
        resp = self.glm.invoke([HumanMessage(content=prompt)])

        if not resp or not resp.content:
            raise ValueError("[AGENT-2] GLM returned empty response")

        parsed = self._parse_selection(resp.content)
        user_context["technique_selection"] = parsed
        logger.info(f"âœ… [AGENT-2] Technique={parsed.get('primary_technique','?')}")
        return user_context

    def _build_prompt(self, ctx: Dict) -> str:
        psych = ctx.get("psychological_analysis", {})
        nlp = ctx.get("nlp_analysis", {})
        cultural = ctx.get("cultural_context", {})

        return f"""You are a therapeutic technique advisor for Indian youth (16-25).
Based on the psychological assessment below, select the best therapeutic approach.
Return ONLY valid JSON (no markdown fences):

{{
  "primary_technique": "<CBT|ACT|MBCT|DBT|MI|Solution-Focused|Person-Centered|Psychoeducation>",
  "therapeutic_approach": "<brief description of how to apply this technique>",
  "activity_recommendations": ["<activity1>", "<activity2>", "<activity3>"],
  "rationale": "<why this technique suits the current situation>"
}}

â”€â”€â”€ ASSESSMENT â”€â”€â”€

Emotional state: {psych.get('emotional_state','')}
Stress categories: {psych.get('stress_categories',[])}
Risk: {psych.get('risk_assessment','low')}
Intervention priority: {psych.get('intervention_priority','supportive')}
Insights: {psych.get('psychological_insights',[])}
Cultural pressures: {psych.get('cultural_pressures','')}

Emotion intensity: {nlp.get('intensity',0):.2f}
Primary emotion: {nlp.get('primary_emotion','unknown')}
Urgency: {nlp.get('urgency_flag', False)}

Language style: {cultural.get('language_style','casual')}
Cultural flags: {cultural.get('cultural_sensitivity_flags',[])}
Formality: {cultural.get('formality_level','medium')}

Consider Indian cultural context: family dynamics, academic pressure, mental health stigma.
Prefer culturally appropriate, practical activities (yoga, journaling, grounding exercises).

JSON:"""

    def _parse_selection(self, raw: str) -> Dict:
        defaults = {
            "primary_technique": "Person-Centered",
            "therapeutic_approach": "Empathetic listening with gentle exploration",
            "activity_recommendations": [],
            "rationale": "",
        }
        try:
            cleaned = re.sub(r"```(?:json)?", "", raw).strip().rstrip("`")
            parsed = json.loads(cleaned)
            for k, v in defaults.items():
                if k not in parsed:
                    parsed[k] = v
            return parsed
        except json.JSONDecodeError:
            logger.warning("[AGENT-2] JSON parse failed, using defaults")
            return defaults


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  7. GLM RESPONSE GENERATOR                                  â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ResponseGenerator:
    """
    Final stage: reads the full UserContext JSON and generates a natural,
    culturally-sensitive, therapeutically-informed companion response.
    """

    SYSTEM_PROMPT = """You are MindMitra, a culturally-aware AI therapeutic companion for Indian youth (16-25).

RESPONSE RULES:
â€¢ Combine psychology expertise with warm, companion-style delivery
â€¢ Match the user's language style (if they use Hindi/Hinglish, mirror appropriately)
â€¢ Apply the selected therapeutic technique naturally â€” do NOT label techniques
â€¢ Reference session memories when relevant to show continuity
â€¢ Be empathetic, non-judgmental, like a caring friend who understands psychology
â€¢ Validate cultural struggles without dismissing traditional values
â€¢ Keep responses conversational â€” concise for casual chat, deeper for heavy topics
â€¢ NEVER include numbered annotations, technique labels in parentheses, or meta-commentary
â€¢ Generate ONLY the natural conversation response"""

    def __init__(self, glm: GLMController):
        self.glm = glm
        logger.info("âœ… [RESPONSE-GEN] Response generator ready")

    def generate(self, user_context: Dict[str, Any]) -> Dict[str, Any]:
        logger.info("ğŸ’¬ [RESPONSE-GEN] Generating therapeutic response...")

        system_msg = SystemMessage(content=self.SYSTEM_PROMPT)
        human_msg = HumanMessage(content=self._build_context(user_context))

        resp = self.glm.invoke([system_msg, human_msg])

        if not resp or not resp.content:
            raise ValueError("[RESPONSE-GEN] GLM returned empty response")

        cleaned = self._clean(resp.content)
        user_context["ai_response"] = cleaned
        user_context["response_generated"] = True
        logger.info(f"âœ… [RESPONSE-GEN] Response ready ({len(cleaned)} chars)")
        return user_context

    def _build_context(self, ctx: Dict) -> str:
        psych = ctx.get("psychological_analysis", {})
        technique = ctx.get("technique_selection", {})
        nlp = ctx.get("nlp_analysis", {})
        cultural = ctx.get("cultural_context", {})
        voice = ctx.get("voice_analysis", {})
        session = ctx.get("session_context", {})

        # Format recent messages for conversation flow
        recent = session.get("recent_messages", [])[-3:]
        conv = "\n".join(
            f"{'User' if m.get('role')=='user' else 'MindMitra'}: {m.get('content','')[:150]}"
            for m in recent
        )

        # Format key memories
        memories = session.get("session_memories", {})
        mem_lines = []
        for mtype in ("procedural", "semantic", "episodic"):
            for m in memories.get(mtype, [])[:3]:
                c = m.get("memory_content", m.get("content", ""))
                mem_lines.append(f"[{mtype}] {c[:100]}")
        mem_block = "\n".join(mem_lines) if mem_lines else ""

        voice_block = ""
        if voice:
            voice_block = f"""
VOICE ANALYSIS:
  Emotional tone: {voice.get('emotional_tone','N/A')}
  Stress level: {voice.get('stress_level','N/A')}
  Speech pace: {voice.get('speech_pace','N/A')}"""

        return f"""PSYCHOLOGICAL ASSESSMENT:
  State: {psych.get('emotional_state','')}
  Stress: {psych.get('stress_categories',[])}
  Priority: {psych.get('intervention_priority','')}
  Insights: {psych.get('psychological_insights',[])}
  Cultural pressures: {psych.get('cultural_pressures','')}

TECHNIQUE:
  Approach: {technique.get('primary_technique','')} â€” {technique.get('therapeutic_approach','')}
  Activities: {technique.get('activity_recommendations',[])}

EMOTION: {nlp.get('primary_emotion','?')} (intensity {nlp.get('intensity',0):.1f}), sentiment={nlp.get('sentiment',{}).get('label','neutral')}
LANGUAGE STYLE: {cultural.get('language_style','casual')}, formality={cultural.get('formality_level','medium')}
CULTURAL FLAGS: {cultural.get('cultural_sensitivity_flags',[])}
{voice_block}

{f'MEMORIES:{chr(10)}{mem_block}' if mem_block else ''}

CONVERSATION:
{conv if conv else '(New conversation)'}

USER'S CURRENT MESSAGE: "{ctx['user_message']}"

Respond naturally as MindMitra:"""

    def _clean(self, text: str) -> str:
        text = text.strip()
        if text.startswith('"') and text.endswith('"'):
            text = text[1:-1]
        if text.startswith("{") or text.startswith("["):
            try:
                p = json.loads(text)
                if isinstance(p, dict) and "content" in p:
                    return p["content"]
            except json.JSONDecodeError:
                pass
        return text.strip()


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  8. MAIN WORKFLOW ORCHESTRATOR                               â•‘
# â•‘     (preserves identical external API)                       â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class MindMitraWorkflow:
    """
    Orchestrates the full pipeline:
      1. Build UserContext JSON
      2. Fetch memories â†’ populate session_context
      3. Groq NLP â†’ populate nlp_analysis
      4. Cultural context â†’ populate cultural_context
      5. GLM Agent 1 (Psychologist) â†’ populate psychological_analysis
      6. GLM Agent 2 (Technique Selector) â†’ populate technique_selection
      7. GLM Response Generator â†’ populate ai_response
      8. Return result in the SAME format as before
    """

    def __init__(self):
        logger.info("ğŸ§  [WORKFLOW] Initialising MindMitra v2 (modular architecture)...")

        # â”€â”€ Supabase â”€â”€
        supabase_url = os.getenv("SUPABASE_URL")
        supabase_key = os.getenv("SUPABASE_KEY")
        if supabase_url and supabase_key:
            self.supabase: Client = create_client(supabase_url, supabase_key)
            logger.info("âœ… [WORKFLOW] Supabase client ready")
        else:
            self.supabase = None
            logger.warning("âš ï¸ [WORKFLOW] Supabase not configured")

        # â”€â”€ Memory system (UNCHANGED) â”€â”€
        google_api_key = os.getenv("GOOGLE_API_KEY")
        try:
            if google_api_key:
                self.memory_system = UniversalMemorySystem(api_key=google_api_key)
                logger.info("âœ… [WORKFLOW] Memory system ready")
            else:
                self.memory_system = None
        except Exception as e:
            self.memory_system = None
            logger.error(f"âŒ [WORKFLOW] Memory system init failed: {e}")

        # â”€â”€ Modules â”€â”€
        self.groq_nlp = GroqNLPModule()
        self.cultural_module = CulturalContextModule(groq_nlp=self.groq_nlp)
        self.glm = GLMController(api_key=google_api_key)
        self.agent_psychologist = PsychologistAnalysisAgent(self.glm)
        self.agent_technique = TechniqueSelectorAgent(self.glm)
        self.response_gen = ResponseGenerator(self.glm)

        # â”€â”€ Background summarisation cache (same as original) â”€â”€
        self._summarization_cache = {}
        self._last_summarization_count = {}

        logger.info("âœ… [WORKFLOW] MindMitra v2 fully initialised\n")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  MEMORY METHODS â€” KEPT IDENTICAL TO ORIGINAL
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def fetch_session_memories(self, session_id: str) -> Dict[str, List[Dict]]:
        """Fetch all memories for a session from database (UNCHANGED from v1)."""
        logger.info(f"ğŸ” [FETCH_MEMORIES] Fetching for session: {session_id}")
        if not self.supabase or not session_id:
            return {"procedural": [], "semantic": [], "episodic": []}

        try:
            response = (
                self.supabase.table("memories")
                .select("*")
                .eq("session_id", session_id)
                .order("created_at", desc=True)
                .execute()
            )

            if not response.data:
                return {"procedural": [], "semantic": [], "episodic": []}

            memories: Dict[str, List] = {"procedural": [], "semantic": [], "episodic": []}

            for row in response.data:
                for memory_type in ("procedural", "semantic", "episodic"):
                    column_name = f"{memory_type}_memories"
                    jsonb_data = row.get(column_name, [])
                    if isinstance(jsonb_data, str):
                        try:
                            jsonb_data = json.loads(jsonb_data)
                        except Exception:
                            jsonb_data = []
                    if isinstance(jsonb_data, list):
                        for mem in jsonb_data:
                            memories[memory_type].append({
                                "memory_content": mem.get("memory_content", mem.get("content", str(mem))),
                                "confidence": mem.get("confidence", mem.get("confidence_level", 0.5)),
                                "created_at": row.get("created_at"),
                                "memory_id": row.get("id"),
                                "importance": mem.get("importance", "medium"),
                                "category": mem.get("category", "general"),
                            })

            total = sum(len(v) for v in memories.values())
            logger.info(f"âœ… [FETCH_MEMORIES] {total} memories (P={len(memories['procedural'])}, S={len(memories['semantic'])}, E={len(memories['episodic'])})")
            return memories

        except Exception as e:
            logger.error(f"âŒ [FETCH_MEMORIES] Error: {e}")
            return {"procedural": [], "semantic": [], "episodic": []}

    def fetch_last_n_messages(self, session_id: str, n: int = 15) -> List[Dict]:
        """Fetch last N unprocessed messages (UNCHANGED from v1)."""
        if not self.supabase or not session_id:
            return []
        try:
            response = (
                self.supabase.table("chat_messages")
                .select("id, role, content, created_at")
                .eq("session_id", session_id)
                .eq("processed_into_memory", False)
                .order("created_at", desc=False)
                .limit(n)
                .execute()
            )
            return [
                {"id": r["id"], "role": r["role"], "content": r["content"], "timestamp": r["created_at"]}
                for r in response.data
            ]
        except Exception as e:
            logger.error(f"âŒ [WORKFLOW] fetch messages error: {e}")
            return []

    def trigger_memory_extraction(self, session_id: str, user_id: str):
        """Trigger memory extraction â€” UNCHANGED from v1."""
        try:
            logger.info("=" * 60)
            logger.info(f"ğŸ§  [MEMORY EXTRACTION] session={session_id}, user={user_id}")
            messages = self.fetch_last_n_messages(session_id, n=15)
            if not messages or not self.memory_system:
                return

            chat_data = {
                "data_type": "chat",
                "user_id": user_id,
                "session_id": session_id,
                "chat_history": messages,
            }

            result = self.memory_system.process_data_to_memories(chat_data)

            memory_record = {
                "user_id": user_id,
                "session_id": session_id,
                "data_type": "chat",
                "procedural_memories": result["memories"].get("procedural", []),
                "semantic_memories": result["memories"].get("semantic", []),
                "episodic_memories": result["memories"].get("episodic", []),
                "memory_summary": {
                    "procedural_count": len(result["memories"].get("procedural", [])),
                    "semantic_count": len(result["memories"].get("semantic", [])),
                    "episodic_count": len(result["memories"].get("episodic", [])),
                    "extraction_timestamp": datetime.now(timezone.utc).isoformat(),
                },
                "source_message_ids": [msg["id"] for msg in messages],
                "metadata": {"message_count": len(messages), "extraction_method": "parallel_llm"},
                "processed_at": datetime.now(timezone.utc).isoformat(),
            }
            self.supabase.table("memories").insert(memory_record).execute()

            message_ids = [msg["id"] for msg in messages]
            if message_ids:
                self.supabase.table("chat_messages").update(
                    {"processed_into_memory": True}
                ).in_("id", message_ids).execute()

            logger.info(f"âœ… [MEMORY EXTRACTION] Done")
            logger.info("=" * 60)

        except Exception as e:
            logger.error(f"âŒ [MEMORY EXTRACTION] Failed: {e}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  CORE PIPELINE
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def process_chat(
        self,
        user_message: str,
        recent_messages: Optional[List] = None,
        conversation_summary: Optional[Dict] = None,
        user_activities: Optional[List] = None,
        user_patterns: Optional[Dict] = None,
        voice_analysis: Optional[Dict] = None,
        user_id: str = "anonymous",
        session_id: str = None,
    ) -> Dict[str, Any]:
        """
        Main processing pipeline â€” SAME SIGNATURE & RETURN FORMAT as original.
        Internally uses the new modular architecture.
        """
        start_time = datetime.now()

        # â”€â”€ 1. Build UserContext JSON â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ctx = create_empty_user_context(user_id, session_id, user_message.strip())
        ctx["voice_analysis"] = voice_analysis or {}
        ctx["session_context"]["recent_messages"] = recent_messages or []
        ctx["session_context"]["conversation_summary"] = conversation_summary or {}
        ctx["session_context"]["user_activities"] = user_activities or []
        ctx["session_context"]["user_patterns"] = user_patterns or {}

        # â”€â”€ 2. Fetch session memories â†’ into ctx â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if session_id:
            ctx["session_context"]["session_memories"] = self.fetch_session_memories(session_id)

        # â”€â”€ 3. Groq NLP emotion/sentiment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        try:
            ctx = self.groq_nlp.analyse(ctx)
        except Exception as e:
            logger.error(f"âŒ [PIPELINE] NLP module error (non-fatal): {e}")

        # â”€â”€ 4. Cultural context analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        try:
            ctx = self.cultural_module.analyse(ctx)
        except Exception as e:
            logger.error(f"âŒ [PIPELINE] Cultural module error (non-fatal): {e}")

        # â”€â”€ 5. GLM Agent 1: Psychologist analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ctx = self.agent_psychologist.run(ctx)

        # â”€â”€ 6. GLM Agent 2: Technique selection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ctx = self.agent_technique.run(ctx)

        # â”€â”€ 7. GLM Response generation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        ctx = self.response_gen.generate(ctx)

        processing_time = (datetime.now() - start_time).total_seconds()

        # â”€â”€ 8. Build output in ORIGINAL FORMAT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        psych = ctx["psychological_analysis"]
        technique = ctx["technique_selection"]

        return {
            "message": ctx["ai_response"],
            "modality": technique.get("primary_technique", "Person-Centered"),
            "confidence": 0.9,
            "processing_time": processing_time,
            "session_insights": {
                "emotional_state": psych.get("emotional_state", ""),
                "stress_categories": psych.get("stress_categories", []),
                "therapeutic_approach": technique.get("primary_technique", ""),
                "cultural_pressures": psych.get("cultural_pressures", ""),
                "language_style": ctx["cultural_context"].get("language_style", ""),
                "psychological_insights": psych.get("psychological_insights", []),
                "coping_assessment": psych.get("coping_assessment", ""),
                "intervention_priority": psych.get("intervention_priority", ""),
                "activity_recommendations": technique.get("activity_recommendations", []),
                # Extra data available in v2
                "nlp_analysis": ctx["nlp_analysis"],
                "cultural_context": ctx["cultural_context"],
                "technique_rationale": technique.get("rationale", ""),
                "performance_metrics": {
                    "context_messages": len(ctx["session_context"]["recent_messages"]),
                    "context_activities": len(ctx["session_context"]["user_activities"]),
                    "has_summary": bool(ctx["session_context"]["conversation_summary"]),
                    "memory_count": sum(
                        len(v) for v in ctx["session_context"]["session_memories"].values()
                    ),
                },
            },
        }


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  9. GLOBAL INSTANCE & ENTRY POINT (UNCHANGED SIGNATURE)      â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_workflow_instance = None


def get_workflow_instance() -> MindMitraWorkflow:
    global _workflow_instance
    if _workflow_instance is None:
        _workflow_instance = MindMitraWorkflow()
    return _workflow_instance


def process_user_chat(
    user_message: str,
    recent_messages: Optional[List] = None,
    conversation_summary: Optional[Dict] = None,
    user_activities: Optional[List] = None,
    user_patterns: Optional[Dict] = None,
    voice_analysis: Optional[Dict] = None,
    user_id: str = "anonymous",
    session_id: str = None,
) -> Dict[str, Any]:
    """Main entry point â€” IDENTICAL SIGNATURE to original v1."""

    logger.info(f"ğŸš€ [ENTRY] MindMitra v2 â€” user={user_id}, session={session_id}")
    start_time = time.time()

    try:
        workflow = get_workflow_instance()
        result = workflow.process_chat(
            user_message, recent_messages, conversation_summary,
            user_activities, user_patterns, voice_analysis, user_id, session_id,
        )
        result["processing_time"] = round(time.time() - start_time, 2)
        result["voice_aware"] = bool(voice_analysis)
        logger.info(f"âœ… [ENTRY] Done in {result['processing_time']}s")
        return result

    except Exception as e:
        logger.error(f"âŒ [ENTRY] Failed after {time.time()-start_time:.2f}s: {e}")
        raise